{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Mood of the News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dependencies\n",
    "import tweepy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# Import and Initialize Sentiment Analyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Twitter API Keys\n",
    "from config import (consumer_key, \n",
    "                    consumer_secret, \n",
    "                    access_token, \n",
    "                    access_token_secret)\n",
    "\n",
    "# Setup Tweepy API Authentication\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: @BBCNews\n",
      "Compound: -0.076\n",
      "Positive: 0.053\n",
      "Neutral: 0.843\n",
      "Negative: 0.104\n",
      "\n",
      "User: @CNN\n",
      "Compound: -0.107\n",
      "Positive: 0.037\n",
      "Neutral: 0.881\n",
      "Negative: 0.082\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Target Search Term\n",
    "target_user = (\"@BBCNews\", \"@CNN\")\n",
    "#\"@CNN\", \"@CBSNews\",\"@FoxNews\", \"@nytimes\"\n",
    "# Array to hold sentiment\n",
    "sentiment_array = []\n",
    "\n",
    "# A list to hold tweet timestamps\n",
    "tweet_times = []\n",
    "\n",
    "# Loop through all target users\n",
    "for target in target_user:\n",
    "\n",
    "    # Variable for holding the oldest tweet\n",
    "    oldest_tweet = None\n",
    "\n",
    "    # Variables for holding sentiments\n",
    "    compound_list = []\n",
    "    positive_list = []\n",
    "    negative_list = []\n",
    "    neutral_list = []\n",
    "    \n",
    "    # Loop through 5 pages of tweets (total 100 tweets)\n",
    "    for x in range(5):\n",
    "        \n",
    "        # Get all tweets from home feed\n",
    "        public_tweets = api.user_timeline(target, page=x)\n",
    "\n",
    "        # Loop through all tweets\n",
    "        for tweet in public_tweets:\n",
    "            \n",
    "            #Get the raw tweet time\n",
    "            raw_time = tweet[\"created_at\"]\n",
    "            tweet_times.append(raw_time)\n",
    "\n",
    "            # Run Vader Analysis on each tweet\n",
    "            results = analyzer.polarity_scores(tweet[\"text\"])\n",
    "            compound = results[\"compound\"]\n",
    "            pos = results[\"pos\"]\n",
    "            neu = results[\"neu\"]\n",
    "            neg = results[\"neg\"]\n",
    "            \n",
    "            # Add each value to the appropriate list\n",
    "            compound_list.append(compound)\n",
    "            positive_list.append(pos)\n",
    "            negative_list.append(neg)\n",
    "            neutral_list.append(neu)\n",
    "            \n",
    "            \n",
    "            \n",
    "    # Print the Averages\n",
    "    print(f\"User: {target}\")\n",
    "    print(f\"Compound: {np.mean(compound_list):.3f}\")\n",
    "    print(f\"Positive: {np.mean(positive_list):.3f}\")\n",
    "    print(f\"Neutral: {np.mean(neutral_list):.3f}\")\n",
    "    print(f\"Negative: {np.mean(negative_list):.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert tweet timestamps to datetime objects that can be manipulated by\n",
    "# Python\n",
    "converted_timestamps = []\n",
    "for raw_time in tweet_times:\n",
    "    # https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior\n",
    "    # http://strftime.org/\n",
    "    converted_time = datetime.strptime(raw_time, \"%a %b %d %H:%M:%S %z %Y\")\n",
    "    converted_timestamps.append(converted_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-29 19:40:45+00:00\n",
      "2017-11-29 17:43:25+00:00\n",
      "2017-11-29 17:35:06+00:00\n",
      "2017-11-29 17:33:19+00:00\n",
      "2017-11-29 17:23:38+00:00\n",
      "2017-11-29 17:18:21+00:00\n",
      "2017-11-29 16:54:49+00:00\n",
      "2017-11-29 16:33:09+00:00\n",
      "2017-11-29 16:16:13+00:00\n",
      "2017-11-29 16:16:13+00:00\n",
      "2017-11-29 15:59:54+00:00\n",
      "2017-11-29 15:32:04+00:00\n",
      "2017-11-29 15:28:26+00:00\n",
      "2017-11-29 15:17:20+00:00\n",
      "2017-11-29 14:35:10+00:00\n",
      "2017-11-29 14:02:34+00:00\n",
      "2017-11-29 13:52:28+00:00\n",
      "2017-11-29 13:37:05+00:00\n",
      "2017-11-29 13:37:05+00:00\n",
      "2017-11-29 12:55:29+00:00\n",
      "2017-11-29 19:40:45+00:00\n",
      "2017-11-29 17:43:25+00:00\n",
      "2017-11-29 17:35:06+00:00\n",
      "2017-11-29 17:33:19+00:00\n",
      "2017-11-29 17:23:38+00:00\n",
      "2017-11-29 17:18:21+00:00\n",
      "2017-11-29 16:54:49+00:00\n",
      "2017-11-29 16:33:09+00:00\n",
      "2017-11-29 16:16:13+00:00\n",
      "2017-11-29 16:16:13+00:00\n",
      "2017-11-29 15:59:54+00:00\n",
      "2017-11-29 15:32:04+00:00\n",
      "2017-11-29 15:28:26+00:00\n",
      "2017-11-29 15:17:20+00:00\n",
      "2017-11-29 14:35:10+00:00\n",
      "2017-11-29 14:02:34+00:00\n",
      "2017-11-29 13:52:28+00:00\n",
      "2017-11-29 13:37:05+00:00\n",
      "2017-11-29 13:37:05+00:00\n",
      "2017-11-29 12:55:29+00:00\n",
      "2017-11-29 12:53:00+00:00\n",
      "2017-11-29 12:38:09+00:00\n",
      "2017-11-29 12:33:32+00:00\n",
      "2017-11-29 12:24:55+00:00\n",
      "2017-11-29 12:19:10+00:00\n",
      "2017-11-29 12:13:27+00:00\n",
      "2017-11-29 12:07:44+00:00\n",
      "2017-11-29 11:51:45+00:00\n",
      "2017-11-29 11:48:58+00:00\n",
      "2017-11-29 11:33:28+00:00\n",
      "2017-11-29 11:23:48+00:00\n",
      "2017-11-29 11:19:00+00:00\n",
      "2017-11-29 10:30:16+00:00\n",
      "2017-11-29 10:29:24+00:00\n",
      "2017-11-29 10:26:16+00:00\n",
      "2017-11-29 10:25:14+00:00\n",
      "2017-11-29 10:16:08+00:00\n",
      "2017-11-29 10:15:40+00:00\n",
      "2017-11-29 09:58:12+00:00\n",
      "2017-11-29 09:38:22+00:00\n",
      "2017-11-29 09:38:03+00:00\n",
      "2017-11-29 08:48:11+00:00\n",
      "2017-11-29 08:29:36+00:00\n",
      "2017-11-29 08:09:16+00:00\n",
      "2017-11-29 07:49:46+00:00\n",
      "2017-11-29 07:38:18+00:00\n",
      "2017-11-29 07:27:23+00:00\n",
      "2017-11-29 07:22:20+00:00\n",
      "2017-11-29 07:12:18+00:00\n",
      "2017-11-29 07:11:16+00:00\n",
      "2017-11-29 06:00:58+00:00\n",
      "2017-11-29 04:03:30+00:00\n",
      "2017-11-29 02:09:23+00:00\n",
      "2017-11-29 02:09:23+00:00\n",
      "2017-11-29 01:59:06+00:00\n",
      "2017-11-29 01:49:19+00:00\n",
      "2017-11-29 01:48:13+00:00\n",
      "2017-11-29 01:43:39+00:00\n",
      "2017-11-29 01:01:20+00:00\n",
      "2017-11-28 22:59:10+00:00\n",
      "2017-11-28 22:51:19+00:00\n",
      "2017-11-28 22:44:06+00:00\n",
      "2017-11-28 22:43:05+00:00\n",
      "2017-11-28 22:42:02+00:00\n",
      "2017-11-28 22:41:23+00:00\n",
      "2017-11-28 22:36:33+00:00\n",
      "2017-11-28 22:32:19+00:00\n",
      "2017-11-28 22:31:02+00:00\n",
      "2017-11-28 22:24:01+00:00\n",
      "2017-11-28 22:19:46+00:00\n",
      "2017-11-28 21:56:20+00:00\n",
      "2017-11-28 21:54:43+00:00\n",
      "2017-11-28 21:49:06+00:00\n",
      "2017-11-28 21:40:29+00:00\n",
      "2017-11-28 21:39:45+00:00\n",
      "2017-11-28 21:37:28+00:00\n",
      "2017-11-28 21:31:00+00:00\n",
      "2017-11-28 21:01:35+00:00\n",
      "2017-11-28 20:51:55+00:00\n",
      "2017-11-28 20:34:10+00:00\n",
      "2017-11-29 20:00:26+00:00\n",
      "2017-11-29 19:50:21+00:00\n",
      "2017-11-29 19:40:21+00:00\n",
      "2017-11-29 19:30:11+00:00\n",
      "2017-11-29 19:20:16+00:00\n",
      "2017-11-29 19:10:07+00:00\n",
      "2017-11-29 19:02:35+00:00\n",
      "2017-11-29 19:00:20+00:00\n",
      "2017-11-29 18:50:05+00:00\n",
      "2017-11-29 18:40:03+00:00\n",
      "2017-11-29 18:32:22+00:00\n",
      "2017-11-29 18:20:15+00:00\n",
      "2017-11-29 18:11:07+00:00\n",
      "2017-11-29 18:00:50+00:00\n",
      "2017-11-29 17:52:06+00:00\n",
      "2017-11-29 17:42:08+00:00\n",
      "2017-11-29 17:30:15+00:00\n",
      "2017-11-29 17:23:33+00:00\n",
      "2017-11-29 17:10:29+00:00\n",
      "2017-11-29 16:51:59+00:00\n",
      "2017-11-29 20:00:26+00:00\n",
      "2017-11-29 19:50:21+00:00\n",
      "2017-11-29 19:40:21+00:00\n",
      "2017-11-29 19:30:11+00:00\n",
      "2017-11-29 19:20:16+00:00\n",
      "2017-11-29 19:10:07+00:00\n",
      "2017-11-29 19:02:35+00:00\n",
      "2017-11-29 19:00:20+00:00\n",
      "2017-11-29 18:50:05+00:00\n",
      "2017-11-29 18:40:03+00:00\n",
      "2017-11-29 18:32:22+00:00\n",
      "2017-11-29 18:20:15+00:00\n",
      "2017-11-29 18:11:07+00:00\n",
      "2017-11-29 18:00:50+00:00\n",
      "2017-11-29 17:52:06+00:00\n",
      "2017-11-29 17:42:08+00:00\n",
      "2017-11-29 17:30:15+00:00\n",
      "2017-11-29 17:23:33+00:00\n",
      "2017-11-29 17:10:29+00:00\n",
      "2017-11-29 16:51:59+00:00\n",
      "2017-11-29 16:40:10+00:00\n",
      "2017-11-29 16:29:04+00:00\n",
      "2017-11-29 16:18:17+00:00\n",
      "2017-11-29 16:05:03+00:00\n",
      "2017-11-29 15:55:09+00:00\n",
      "2017-11-29 15:45:10+00:00\n",
      "2017-11-29 15:35:03+00:00\n",
      "2017-11-29 15:25:07+00:00\n",
      "2017-11-29 15:15:04+00:00\n",
      "2017-11-29 15:05:08+00:00\n",
      "2017-11-29 14:55:06+00:00\n",
      "2017-11-29 14:45:07+00:00\n",
      "2017-11-29 14:35:08+00:00\n",
      "2017-11-29 14:25:06+00:00\n",
      "2017-11-29 14:15:07+00:00\n",
      "2017-11-29 14:05:07+00:00\n",
      "2017-11-29 13:55:20+00:00\n",
      "2017-11-29 13:41:16+00:00\n",
      "2017-11-29 13:30:24+00:00\n",
      "2017-11-29 13:19:56+00:00\n",
      "2017-11-29 13:07:44+00:00\n",
      "2017-11-29 12:59:25+00:00\n",
      "2017-11-29 12:50:50+00:00\n",
      "2017-11-29 12:48:03+00:00\n",
      "2017-11-29 12:37:21+00:00\n",
      "2017-11-29 12:37:03+00:00\n",
      "2017-11-29 12:31:51+00:00\n",
      "2017-11-29 12:29:47+00:00\n",
      "2017-11-29 12:22:38+00:00\n",
      "2017-11-29 12:10:06+00:00\n",
      "2017-11-29 12:09:51+00:00\n",
      "2017-11-29 12:00:24+00:00\n",
      "2017-11-29 11:57:57+00:00\n",
      "2017-11-29 11:50:06+00:00\n",
      "2017-11-29 11:48:06+00:00\n",
      "2017-11-29 11:40:09+00:00\n",
      "2017-11-29 11:30:07+00:00\n",
      "2017-11-29 11:18:44+00:00\n",
      "2017-11-29 11:08:38+00:00\n",
      "2017-11-29 11:01:33+00:00\n",
      "2017-11-29 10:51:33+00:00\n",
      "2017-11-29 10:38:27+00:00\n",
      "2017-11-29 10:28:29+00:00\n",
      "2017-11-29 10:14:57+00:00\n",
      "2017-11-29 10:01:26+00:00\n",
      "2017-11-29 09:48:12+00:00\n",
      "2017-11-29 09:13:25+00:00\n",
      "2017-11-29 08:59:03+00:00\n",
      "2017-11-29 08:59:03+00:00\n",
      "2017-11-29 08:49:46+00:00\n",
      "2017-11-29 08:40:15+00:00\n",
      "2017-11-29 08:25:30+00:00\n",
      "2017-11-29 08:09:33+00:00\n",
      "2017-11-29 08:01:05+00:00\n",
      "2017-11-29 07:58:31+00:00\n",
      "2017-11-29 07:48:37+00:00\n",
      "2017-11-29 07:40:08+00:00\n",
      "2017-11-29 07:32:04+00:00\n",
      "2017-11-29 07:22:09+00:00\n",
      "2017-11-29 07:09:11+00:00\n"
     ]
    }
   ],
   "source": [
    "for tweet in converted_timestamps:\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "print(len(converted_timestamps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1. Need to get time stamps of the 100 tweets \"Tweets Ago\".\n",
    "#2. Need to put tweet in df with time stamp and the compound sentiment analysis. \n",
    "#3. Graph the 100 tweets from each news source with a timestamp \"Tweets Ago\". "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
